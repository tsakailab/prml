{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/ex_MNIST_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCAによる低次元化（dimensionality reduction by principal component analysis）\n",
        "\n",
        "主成分分析（PCA）でデータを低次元空間に埋め込む圧縮，再構成による近似の性質を観察しましょう．\n",
        "\n",
        "参考資料：\n",
        "- [Principal component analysis@Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
        "- [Principal component analsys@scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
        "\n",
        "----\n",
        "\n",
        "氏名：\n",
        "\n",
        "学生番号：\n",
        "\n",
        "----\n",
        "## 基本課題（必須）\n",
        "\n",
        "    1. Fashion-MNISTの 0，2，4，6 の 4 クラスからなる画像集合を主成分分析で低次元化したとき，\n",
        "       上位の主成分はどのような画像の違いを表現していますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    2. Fashion-MNISTの 0，2，4，6 の 4 クラスからなる画像集合を上位の主成分を用いて圧縮・再構成したとき，\n",
        "       画像のどのような特徴が主に再構成され易い・され難いですか．また，その原因を考えてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    3. 再構成誤差は，主成分の数に対してどのように依存しますか．特に，主成分の数が小さ過ぎる・大き過ぎるとき，\n",
        "       どのような原因で訓練画像の再構成誤差が大きく・小さくなると考えられますか．\n",
        "       ノイズの大小や訓練データ数による違いも踏まえて考察してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    4.その他，気づいたこと，調べたことを書いてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "----\n",
        "発展課題（任意）がこのノートブックの後半にあります．"
      ],
      "metadata": {
        "id": "AH8-0BiKPDuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データ集合を取得します．"
      ],
      "metadata": {
        "id": "tC7fn8ijeihu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
        "#tvds = datasets.MNIST('./data', download=True, train=True)\n",
        "\n",
        "# [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "tvds = datasets.FashionMNIST('./data', download=True, train=True)\n",
        "\n",
        "# [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)\n",
        "#tvds = datasets.KMNIST('./data', download=True, train=True)\n",
        "\n",
        "Ximages_all, y_all = tvds.data.numpy(), tvds.targets.numpy()\n",
        "\n",
        "# [EMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.EMNIST.html)\n",
        "#tvds = datasets.EMNIST('./data', download=True, train=True, split='letters')\n",
        "#Ximages_all, y_all = tvds.data.transpose_(-1,-2).numpy(), tvds.targets.numpy() - 1"
      ],
      "metadata": {
        "id": "pBKK4QZN1eCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ☆画像数とサイズを確認します（画像を加工したい場合はこのセルを編集して利用してください）．\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian\n",
        "from skimage.exposure import equalize_hist as eh\n",
        "from skimage.transform import resize\n",
        "\n",
        "Ximages = Ximages_all.copy()\n",
        "y = y_all.copy()\n",
        "\n",
        "'''\n",
        "* blurring (https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian)\n",
        "* histogram equalization (https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_equalize.html)\n",
        "* resize images (https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html)\n",
        "'''\n",
        "#Ximages = gaussian(np.float32(Ximages_all.transpose((1,2,0))), sigma=1.0, multichannel=True).transpose(2,0,1)\n",
        "#Ximages = eh(Ximages_all.transpose((1,2,0))).transpose(2,0,1) * 255\n",
        "#height, width = 8, 8; Ximages = resize(np.float32(Ximages_all.transpose((1,2,0))), (height, width)).transpose(2,0,1)\n",
        "\n",
        "''' simulate noisy images '''\n",
        "#Ximages = Ximages + np.random.rand(*Ximages.shape) * 200; Ximages = np.clip(Ximages,a_min=0,a_max=255)\n",
        "\n",
        "Ximages = np.uint8(Ximages)\n",
        "print(\"(#images, height, width)\", Ximages.shape)\n",
        "print(Ximages.dtype, \", max. pixel value = \", Ximages.max())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9w91jXrgIKuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST class labels\n",
        "\n",
        "| [MNIST](http://yann.lecun.com/exdb/mnist/) | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
        "| - | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal |  Shirt | Sneaker | Bag | Ankle boot |\n",
        "| [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) | お | き | す | つ | な | は |  ま | や | れ | を |"
      ],
      "metadata": {
        "id": "b85HjGXpQuO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 画像を表示する関数 `implot` を定義し，画像を例示します．\n",
        "%matplotlib inline\n",
        "!wget -q -N https://gist.githubusercontent.com/tsakai-g/c54a8fa059f9c655290586ec1cc2ec7b/raw/implot.py\n",
        "%run implot.py\n",
        "\n",
        "print(\"%d images in total\" % len(y))\n",
        "p = np.random.permutation(len(y))\n",
        "implot(Ximages[p], y[p], num=16, vmin=0, vmax=255)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ivCIQfztnbuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76r4Ic8VAj_g"
      },
      "source": [
        "### クラスを抜粋したい場合（さもなくば実行しなくてよいです）\n",
        "- `classes` でクラスを指定します．例：`classes = [1,4,7,9]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVowqB1Z_Pcs"
      },
      "source": [
        "classes = [0,2,4,6]  # choose from 0 to 9\n",
        "\n",
        "classes = np.unique(classes)\n",
        "is_in_classes = list(map(lambda c: y == c, classes))\n",
        "Ximages = Ximages[np.logical_or.reduce(is_in_classes)]\n",
        "y = y[np.logical_or.reduce(is_in_classes)]\n",
        "\n",
        "p = np.random.permutation(len(y))\n",
        "implot(Ximages[p], y[p], num=20, vmin=0, vmax=255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ☆訓練データと検証データに分けます．\n",
        "- 画像を `Ximages_train` と `Ximages_val` に分けます．それぞれ `n_train` 枚と `n_val` 枚の画像です．\n",
        "- `Ximages_train` と `Ximages_val` を，それぞれ shape が `(n_train, 画素数)`，`(n_val，画素数)` の NumPy 配列にしたものが `X_train`，`X_val` です．"
      ],
      "metadata": {
        "id": "H9BCD7UEZxet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_size = 0.8\n",
        "\n",
        "# split the data into training and validation sets\n",
        "Ximages_train, Ximages_val, y_train, y_val = train_test_split(Ximages, y, train_size=train_size)\n",
        "\n",
        "n_train = len(Ximages_train)\n",
        "n_val = len(Ximages_val)\n",
        "\n",
        "# reshape to 28*28=784-dimensional feature vectors\n",
        "X_train = np.reshape(Ximages_train, (Ximages_train.shape[0], -1))   # (n_train, 784)\n",
        "X_val = np.reshape(Ximages_val, (Ximages_val.shape[0], -1))         # (n_val, 784)\n",
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_val.shape = \", X_val.shape)"
      ],
      "metadata": {
        "id": "Zxtu4nWQZvtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ★訓練データに対して主成分分析を実行します．\n",
        "- [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)を使用して，標準化の前処理を施します．`X_train` を標準化したものが `Xs_train` です．この標準化に従って `X_val` を標準化したものが `Xs_val` です．\n",
        "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)を使用します．引数（特に`n_components`）の仕様とデフォルトの値を確認してください．\n",
        "- `model.fit` は，標本平均 $\\hat{\\boldsymbol{\\mu}}$ と，$k=$ `n_components` 本の主成分（分散共分散行列の固有ベクトル）$\\boldsymbol{U}_{:k}$ を計算します．\n",
        "- クラスラベル `y_train` と `y_val` は主成分分析に必要ありません．"
      ],
      "metadata": {
        "id": "3cy3uyS35LYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xs_train = scaler.fit_transform(X_train)\n",
        "Xs_val = scaler.transform(X_val)\n",
        "\n",
        "n_components=100\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA(n_components=n_components)\n",
        "model.fit(Xs_train)"
      ],
      "metadata": {
        "id": "7-A2ITIiuqQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 標本平均 $\\hat{\\boldsymbol{\\mu}}$ および主成分 $\\boldsymbol{U}_{:k}$ を可視化します．\n",
        "- 標本平均は `model.mean_` です．標準化から0～255の画素値に戻して表示します．\n",
        "- 正を赤，負を青の画素値とする画像として各主成分を表示します．"
      ],
      "metadata": {
        "id": "a7EOHIGHJYKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = Ximages.shape[1],Ximages.shape[2]\n",
        "implot(scaler.inverse_transform(model.mean_.reshape(1,-1)).reshape(-1,h,w), vmin=0, vmax=255)\n",
        "implot(model.components_.reshape(-1,h,w), num=min(20,n_components))"
      ],
      "metadata": {
        "id": "l62M_UeE14oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主成分分析でデータを低次元化します．\n",
        "- `Xs_train` と `Xs_val` を低次元化したものが `Z_train` と `Z_val` です．`model.transform` はデータ $\\boldsymbol x$ から $k$ 次元ベクトル $\\boldsymbol z$ を次式で計算します．\n",
        "$\\boldsymbol{z}=\\boldsymbol{U}_{:k}^\\top(\\boldsymbol{x}-\\hat{\\boldsymbol{\\mu}})$"
      ],
      "metadata": {
        "id": "wJ5zQGQTgMDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z_train = model.transform(Xs_train)\n",
        "Z_val = model.transform(Xs_val)"
      ],
      "metadata": {
        "id": "A9x3n9YUwW7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `Z_train` と `Z_val` の第1，2，3成分を，ラベル `y` で色分けしてプロットします．<p>3次元データ X を y で色分けして立方体内にプロットする関数 `plot_embedding3d(X, y)` を定義します．</p>\n",
        "%matplotlib inline\n",
        "!wget -q -N https://gist.githubusercontent.com/tsakai-g/a0730f3692dd915a19a8e7598806e1fe/raw/plot_embedding3d.py\n",
        "%run plot_embedding3d.py"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VF1Cwp-iWDZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全部表示すると多いので，ランダムに n 個だけ表示する．\n",
        "nt = min(300, n_train)\n",
        "pt = np.random.choice(X_train.shape[0], nt, replace=False)\n",
        "plot_embedding3d(Z_train[pt], y_train[pt])\n",
        "\n",
        "nv = min(300, n_val)\n",
        "pv = np.random.choice(Z_val.shape[0], nv, replace=False)\n",
        "plot_embedding3d(Z_val[pv], y_val[pv])"
      ],
      "metadata": {
        "id": "8R0hwrK2WyYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST class labels\n",
        "\n",
        "| [MNIST](http://yann.lecun.com/exdb/mnist/) | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
        "| - | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal |  Shirt | Sneaker | Bag | Ankle boot |\n",
        "| [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) | お | き | す | つ | な | は |  ま | や | れ | を |"
      ],
      "metadata": {
        "id": "yukS41WxzDLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 主成分の数 $k$ とノイズに対する再構成の依存性を調べます．\n",
        "- `X_train` と `X_val` にノイズを加えたデータを `Xn_train` ， `Xn_val` とします．\n",
        "- `Xn_train` と `Xn_val` を標準化後，主成分分析で `Z_train` と `Z_val` に低次元化します．\n",
        "- $\\hat{\\boldsymbol{x}}=\\boldsymbol{U}_{:k}\\boldsymbol{z}_{:k}+\\hat{\\boldsymbol{\\mu}}$ を計算して返す関数 `inverse_transform` を定義します．`k=n_components` のとき，`inverse_transform(model, Z, n_components)` は [`model.inverse_transform(Z)`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform) と同じです．\n",
        "- `k` を指定して再構成し，ノイズを加えていない元画像と再構成画像を比較します．"
      ],
      "metadata": {
        "id": "SaLIf_PB5mgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add noise\n",
        "noise_std = 50 # [0,255]\n",
        "Xn_train = X_train + np.random.randn(*X_train.shape) * noise_std\n",
        "Xn_val = X_val + np.random.randn(*X_val.shape) * noise_std"
      ],
      "metadata": {
        "id": "O3kxBOcEWG_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xs_train = scaler.fit_transform(Xn_train)\n",
        "Xs_val = scaler.transform(Xn_val)\n",
        "\n",
        "n_components=300\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA(n_components=n_components, svd_solver='full').fit(Xs_train)\n",
        "\n",
        "Z_train = model.transform(Xs_train)\n",
        "Z_val = model.transform(Xs_val)"
      ],
      "metadata": {
        "id": "DRVb1Y5G9AyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_transform(model, Z, k):\n",
        "    UkT = model.components_[:k]\n",
        "    mu = model.mean_\n",
        "    Xr = Z[:,:k].dot(UkT) + mu\n",
        "    return Xr\n",
        "\n",
        "def show_reconst(model, Z, k, X, scaler, num=20):\n",
        "    Xr = inverse_transform(model, Z, k)\n",
        "    Xr = scaler.inverse_transform(Xr)\n",
        "    implot(Xr.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "    implot((Xr - X).reshape(-1,h,w), num=num)"
      ],
      "metadata": {
        "id": "Pwt7NPHo8ec2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5   # must be no greater than n_components\n",
        "\n",
        "num = 20\n",
        "print(\"Train: original (1st row), original with noise (2nd row), reconstructed (3rd row), and error [-255(blue),255(red)] (4th row).\")\n",
        "implot(X_train.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "implot(Xn_train.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "show_reconst(model, Z_train, k, X_train, scaler, num=num)\n",
        "\n",
        "plt.show();print(\"\\n\")\n",
        "\n",
        "print(\"Val: original (1st row), original with noise (2nd row), reconstructed (3rd row), and error [-255(blue),255(red)] (4th row).\")\n",
        "implot(X_val.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "implot(Xn_val.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "show_reconst(model, Z_val, k, X_val, scaler, num=num)"
      ],
      "metadata": {
        "id": "Cng0cvIY_XAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $k$ に対する再構成誤差を調べます．"
      ],
      "metadata": {
        "id": "DDeYH17DgsA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_error(model, Z, k, X, scaler):\n",
        "    Xr = inverse_transform(model, Z, k)\n",
        "    Xr = scaler.inverse_transform(Xr)\n",
        "    Err = Xr - X\n",
        "    mse = np.sqrt(np.mean(Err**2))\n",
        "    std = np.std(Err)\n",
        "    return mse, std\n",
        "\n",
        "k_components = np.arange(5, n_components, 10)\n",
        "MSE_train, MSE_val = [], []\n",
        "for k in k_components:\n",
        "    mse, _ = compute_error(model, Z_train, k, X_train, scaler)\n",
        "    MSE_train.append(mse)\n",
        "    mse, _ = compute_error(model, Z_val, k, X_val, scaler)\n",
        "    MSE_val.append(mse)\n",
        "\n",
        "plt.plot(k_components, MSE_train, '.', label=\"train\")\n",
        "plt.plot(k_components, MSE_val, '+', label=\"val\")\n",
        "plt.ylim([0,None])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3URduAl70i6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "## 発展課題（任意）\n",
        "\n",
        "    1. PCAを実装してください．\n",
        "\n",
        "（回答は「★PCAの実装」に書いてください）\n",
        "\n",
        "    2. 特異値分解（singular value decomposition; SVD）について調査し，\n",
        "       標本分散共分散行列の固有ベクトルが SVD で計算できることを説明してください．\n",
        "\n",
        "（ここに回答を書いてください）"
      ],
      "metadata": {
        "id": "2QsruOf_mYxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ★PCAの実装\n",
        "\n",
        "`myPCA`という名のクラスとして実装しましょう．\n",
        "[sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) に似た仕様とします．\n",
        "\n",
        "訓練データ `X_train` から主成分を計算したあと，`X_val` を低次元化するには，\n",
        "```\n",
        "    model = myPCA(n_estimators=300)\n",
        "    model.fit(X_train)\n",
        "    Z_val = model.transform(X_val)\n",
        "```\n",
        "のように使うことを想定します．\n",
        "- 訓練データの次元数`d`，個数`n`の場合，`X_train` は shape が `(n,d)` のNumPy配列です．\n",
        "- `myPCA` の引数 `n_components` は主成分（固有ベクトル）の数です．"
      ],
      "metadata": {
        "id": "cwFEAhOawTvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import svd\n",
        "\n",
        "class myPCA:\n",
        "\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components_ = None\n",
        "        self.mean_ = None\n",
        "        self.singular_values_ = None\n",
        "\n",
        "    def fit(self, X):                                       # X(n,d)\n",
        "        # X の標本平均を計算します．\n",
        "        self.mean_ = ''' np.mean() を使う '''\n",
        "\n",
        "        # 標本分散共分散行列の固有ベクトルを得ます．\n",
        "        _, sv, ev = svd(X - self.mean_, full_matrices=False)\n",
        "        self.components_ = ev[0:self.n_components]\n",
        "        self.singular_values_ = sv[0:self.n_components]     # (n_components,d)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.mean_ is not None:\n",
        "            X_transformed = ''' self.mean_ と self.components を用いて X を低次元化します '''\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "rhVKAwqrzT8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "お疲れさまでした．"
      ],
      "metadata": {
        "id": "jBz0cgBO0qZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 付録：[MedMNIST](https://github.com/MedMNIST/MedMNIST)を使う場合<p>以下を実行後，このJupyter Notebook前半の「☆画像数とサイズを確認します（画像を加工したい場合はこのセルを編集して利用してください）．」以降を実行できます．\n",
        "!pip install -q medmnist\n",
        "import medmnist\n",
        "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "19n5ZTkG1eH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'pneumoniamnist', 'chestmnist', 'octmnist', 'breastmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist'\n",
        "data_flag = 'pneumoniamnist'\n",
        "DataClass = getattr(medmnist, medmnist.INFO[data_flag]['python_class'])\n",
        "\n",
        "tvds = DataClass(split='train', download=True)\n",
        "#tvds = DataClass(split='test', download=True)\n",
        "\n",
        "print(tvds)\n",
        "Ximages_all, y_all = tvds.imgs, tvds.labels.ravel()"
      ],
      "metadata": {
        "id": "2JWf5Htq1eLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}