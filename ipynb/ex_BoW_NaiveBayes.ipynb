{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/ex_BoW_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UTNIg2oEn2n8"
      },
      "cell_type": "markdown",
      "source": [
        "# BoW表現による文書の単純ベイズ識別（naive Bayes classification with BoW）\n",
        "\n",
        "----\n",
        "\n",
        "氏名：\n",
        "\n",
        "学生番号：\n",
        "\n",
        "----\n",
        "基本課題（必須）\n",
        "\n",
        "    1. 文書のどのような表現が Bag of Words と呼ばれていますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    2. Example 2とExample 3は感情分析（sentiment analysis）のデータセットです．\n",
        "       識別の結果が真陽性（TP），真陰性（TN），偽陽性（FP），偽陰性（FN）にそれぞれ該当する文例を検証データの文書からひとつずつ示し，\n",
        "       各クラスに対してどの単語の確率が最も高かったか調べてください．\n",
        "\n",
        "\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    3. 単純ベイズ識別の実装を完成させてください．事後確率最大のクラスを探すために，\n",
        "       確率の積ではなく，確率の対数の和を計算するのはなぜですか．\n",
        "\n",
        "（ここに回答を書いてください）  \n",
        "（実装はこのファイルの後半「★単純ベイズ識別の実装」に書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    4.その他，気づいたこと，調べたことを書いてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "発展課題（任意）がこのノートブックの後半にあります．"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Bag-of-words (BoW) representation](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
        "\n",
        "[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)を使います．\n"
      ],
      "metadata": {
        "id": "ZVWF1x3eNp9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())  # print(X)\n",
        "\n",
        "# # Bag-of-n-grams are also available\n",
        "# vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "# X2 = vectorizer2.fit_transform(corpus)\n",
        "# vectorizer2.get_feature_names_out()\n",
        "# print(\"N-grams:\", vectorizer2.get_feature_names_out())\n",
        "# print(X2.toarray())  # print(X2)"
      ],
      "metadata": {
        "id": "Chk1XhtuLfl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単純ベイズ識別で文書を分類しましょう．"
      ],
      "metadata": {
        "id": "fqrEmYx8Qeij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実験用のテキストデータを入手します．\n",
        "以下の Example からひとつ選んで実行してください．\n",
        "- 入手に成功すると，変数 `text_col_name`，`target_col_name`，`df` が作られます．\n",
        "- `df` は入手したデータのデータフレーム（pandas.DataFrame）です．\n",
        "- `text_col_name` は，`df` に格納されているテキストの列名です．\n",
        "- `target_col_name` はクラスラベルと見なす列名です．"
      ],
      "metadata": {
        "id": "jwxrfb4rAgPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 1: SMS Spam Collection Data Set<p>https://archive.ics.uci.edu/ml/datasets/sms+spam+collection</p>\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "import requests\n",
        "res = requests.get(url)\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "z = zipfile.ZipFile(io.BytesIO(requests.get(url).content))\n",
        "\n",
        "import pandas as pd\n",
        "with z.open(z.namelist()[0], 'r') as file:\n",
        "    df = pd.read_csv(io.BytesIO(file.read()), sep='\\t', header=None, names=['label', 'text'])\n",
        "\n",
        "text_col_name = 'text'      # X\n",
        "target_col_name = 'label'   # y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TR_taIW-2BEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 2: Google Play store apps reviews<p>https://github.com/Hrd2D/Sentiment-analysis-on-Google-Play-store-apps-reviews/</p>\n",
        "\n",
        "url = \"https://github.com/Hrd2D/Sentiment-analysis-on-Google-Play-store-apps-reviews/raw/master/google_play_store_apps_reviews_training.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(url)\n",
        "df.replace({'polarity': {0: 'neg', 1: 'pos'}}, inplace=True)\n",
        "\n",
        "text_col_name = 'review'        # X\n",
        "target_col_name = 'polarity'    # y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nzjQBIW6Uvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 3: Movie Review Data<p>http://www.cs.cornell.edu/people/pabo/movie-review-data/<br>http://www.nltk.org/nltk_data/</p>\n",
        "\n",
        "import nltk\n",
        "\n",
        "# nltk.download('all', halt_on_error=False)\n",
        "corpus_name = 'movie_reviews'   # http://www.nltk.org/nltk_data/\n",
        "nltk.download(corpus_name, halt_on_error=False, download_dir='./', quiet=True);\n",
        "\n",
        "from sklearn.datasets import load_files\n",
        "corpus = load_files('./corpora/' + corpus_name, encoding='utf-8')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame([corpus.data, corpus.target]).T\n",
        "df.set_axis(['text', 'label'], axis=1, inplace=True)\n",
        "df.replace({'label': {0: corpus.target_names[0], 1: corpus.target_names[1]}}, inplace=True)\n",
        "\n",
        "text_col_name = 'text'          # X\n",
        "target_col_name = 'label' # y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JqvxbC9YLgMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 4: Microsoft support ticket classification<p>https://github.com/karolzak/support-tickets-classification#21-main-challenge-and-initial-assumptions</p>\n",
        "\n",
        "url = \"https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/datasets/all_tickets.csv?sp=r&st=2021-06-07T14:36:30Z&se=2022-12-30T23:36:30Z&spr=https&sv=2020-02-10&sr=b&sig=Za0%2Fgbe%2FanVblbcYsCdQS5zTS5%2B17QKESzlbEXPp2KE%3D\"\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "text_col_name = 'body'          # X\n",
        "target_col_name = 'ticket_type' # y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4hqiWExTBX76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 5: Fake News Detection Datasets<p>https://www.uvic.ca/ecs/ece/isot/datasets/fake-news/index.php<br>https://www.uvic.ca/engineering/ece/isot/assets/docs/ISOT_Fake_News_Dataset_ReadMe.pdf<br>https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset<br>https://www.politifact.com/</p>\n",
        "\n",
        "url = \"https://docs.google.com/uc?export=download&confirm=no_antivirus&id=1IoTRrJNDJqvaG3hnUpnHQyGvPAJbO8y3\"\n",
        "\n",
        "import requests\n",
        "res = requests.get(url)\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "z = zipfile.ZipFile(io.BytesIO(requests.get(url).content))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# read multiple csv files in a zipfile\n",
        "dfs = []\n",
        "for filename in z.namelist():\n",
        "    df = pd.read_csv(io.BytesIO(z.open(filename, 'r').read()))\n",
        "    df['label'] = filename[:-4]     # remove '.csv'\n",
        "    dfs.append(df)\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# # read Fake.csv and True.csv directly\n",
        "# import pandas as pd\n",
        "# dff = pd.read_csv(\"https://github.com/FavioVazquez/fake-news/raw/master/data/Fake.csv\")\n",
        "# dff['label'] = 'Fake'\n",
        "# dft = pd.read_csv(\"https://github.com/FavioVazquez/fake-news/raw/master/data/True.csv\")\n",
        "# dft['label'] = 'True'\n",
        "# df = pd.concat([dff, dft], ignore_index=True)\n",
        "# del dff, dft\n",
        "\n",
        "text_col_name = 'text'      # X\n",
        "target_col_name = 'label'   # y"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CqVx4yLey9B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 入手したテキストを例示します．\n",
        "- 実行するごとにランダムに4例（データフレームの4行分）を出力します．\n",
        "- `text_col_name` と `target_col_name` に該当する列があることを確認してください．\n",
        "- 入力のテキストや，出力のクラスラベルとして使用したい別の列があれば，それぞれ `text_col_name` と `target_col_name` に指定してよいです．\n"
      ],
      "metadata": {
        "id": "YvO6IEMGQz_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text_col_name = '????'     # X as input\n",
        "#target_col_name = '????'   # y as output\n",
        "\n",
        "print(\"Column name of texts: text_col_name =\", text_col_name, \"  Target labels: target_col_name =\", target_col_name)\n",
        "print(df[target_col_name].value_counts())\n",
        "\n",
        "pd.set_option('max_colwidth', 1000)\n",
        "display(df.sample(4))"
      ],
      "metadata": {
        "id": "LMDYj6t16T3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前処理：句読点や括弧，数字の除去\n",
        "このセルの実行は省略してもよいです．実行したら，前処理の前後の数例を比較しましょう．"
      ],
      "metadata": {
        "id": "PUBOJ60PiOmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.head())\n",
        "\n",
        "towhite = set([',', '.', '\\n'])\n",
        "for c in towhite:\n",
        "    whiter = lambda text: text.replace(c, ' ')\n",
        "    df[text_col_name] = df[text_col_name].apply(whiter)\n",
        "\n",
        "import string\n",
        "punctuation_remover = lambda text: ''.join([char for char in text if char not in string.punctuation])\n",
        "df[text_col_name] = df[text_col_name].apply(punctuation_remover)\n",
        "\n",
        "number_remover = lambda text: ''.join([i for i in text if not i.isdigit()])\n",
        "df[text_col_name] = df[text_col_name].apply(number_remover)\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "f9UkV2i1iHdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前処理：[stop words](https://en.wikipedia.org/wiki/Stop_word)および特定の単語の除去\n",
        "このセルの実行は省略してもよいです．実行したら，前処理の前後の数例を比較しましょう．"
      ],
      "metadata": {
        "id": "aV6TDzbijxg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.head())\n",
        "\n",
        "from nltk.corpus.reader.wordnet import WordNetICCorpusReader\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True);\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# remove stop words\n",
        "import re\n",
        "stops = set(stopwords.words('english'))\n",
        "stopwords_remover = lambda text: ' '.join([word for word in re.split(\" |(?<![0-9])[.,](?![0-9])\", text) if word.lower() not in (stops)])\n",
        "df[text_col_name] = df.apply(lambda x: stopwords_remover(x[text_col_name]), axis=1)\n",
        "\n",
        "# further remove words that PARTIALLY matches with any string in word_to_remove\n",
        "words_to_remove = ['Reuters', 'Donald', 'Trump', 'Biden']\n",
        "pat = r'\\w*({})\\w* ?'.format('|'.join(words_to_remove))     # regular expression of strings that partially matches any word to remove\n",
        "df[text_col_name] = df[text_col_name].str.replace(pat, '', regex=True)\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "_Y597k0Z6MdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データを分割し，BoW表現へ変換します．"
      ],
      "metadata": {
        "id": "m_3E90felvnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dfX, dfX_val, y, y_val = train_test_split(df[text_col_name], df[target_col_name], train_size=0.8)\n",
        "\n",
        "import numpy as np\n",
        "y = y.astype(str)\n",
        "y_val = y_val.astype(str)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import regex\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# analyzer = lambda text: [(yield word) for word in regex.findall(r'\\w{3,}', text)]   # extract words of at least 3 letters\n",
        "# CountVectorizer(analyzer=analyzer)\n",
        "\n",
        "\n",
        "X = vectorizer.fit_transform(dfX)  # (number of docs, number of words)\n",
        "print(\"#words =\", X.shape[1], \"  vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "X_val = vectorizer.transform(dfX_val)  # (number of docs, number of words)"
      ],
      "metadata": {
        "id": "_-_50aC0S0Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単純ベイズ識別を実行します．\n",
        "sklearn.naive_bayes: Naive Bayes [scikit-learnが提供している単純ベイズ識別](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)を確認しましょう．\n"
      ],
      "metadata": {
        "id": "OLakmwlBpaJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "ih9vXtINN9Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 混同行列（行：正解，列：予測）\n",
        "from sklearn import metrics\n",
        "cm = metrics.confusion_matrix(y_val, model.predict(X_val))\n",
        "print(cm)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, model.predict(X_val)))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OEW60TwfE4Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 特定の文書について，識別結果および確率が上位の単語を観察します．<p>このセルでは上位の単語を表示する関数 `show_topk_words` を定義します．</p>\n",
        "def show_topk_words(model, x_in, topk=3):\n",
        "    xlogp = np.tile(x_in.toarray(),(2,1)) * model.feature_log_prob_\n",
        "    print(\"Top\", topk, \"words with the highest probabilities:\")\n",
        "    print(\"(word, word frequency * log likelihood, log likelihood)\")\n",
        "    for cid in range(len(model.classes_)):\n",
        "        topk_word_ids = np.argsort(xlogp[cid])[:topk][::-1]\n",
        "        topk_words = vectorizer.get_feature_names_out()[topk_word_ids]\n",
        "        topk_probs = model.feature_log_prob_[cid][topk_word_ids]\n",
        "        topk_xprobs = xlogp[cid][topk_word_ids]\n",
        "        print(\"as the class\", model.classes_[cid])\n",
        "        for w, xprob, prob in zip(topk_words, topk_xprobs, topk_probs):\n",
        "            print(\"\\t(%s, %1.2e, %1.2e) \" %(w, np.exp(xprob), np.exp(prob)), end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "A9pA4nyhePmx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytext = [\"I am very interested in the lectures of Prof. T. Sakai. So I would like to thank him by reviewing his lecture. I will try my best to write a nice review which is understandable to others and can be used to introduce to people.\"]\n",
        "\n",
        "x_in = vectorizer.transform(mytext)\n",
        "print(model.predict(x_in)[0])\n",
        "\n",
        "show_topk_words(model, x_in, topk=3)"
      ],
      "metadata": {
        "id": "oyr4Gl2orxec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_id = 5\n",
        "\n",
        "dfx = dfX_val.iloc[text_id]\n",
        "x_in = X_val[text_id]\n",
        "y_true = y_val.iloc[text_id]\n",
        "\n",
        "print(\"Text:\", \"-\"*14)\n",
        "print(dfx)\n",
        "print(\"-\"*20)\n",
        "print(\"is labeled as\", y_true, \"predicted as\", model.predict(x_in)[0])\n",
        "print()\n",
        "\n",
        "show_topk_words(model, x_in, topk=3)"
      ],
      "metadata": {
        "id": "ghsd5vrGMqgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ★単純ベイズ識別の実装\n",
        "\n",
        "`myMultinomialNB` という名のクラスとして実装しましょう．sklearnの[`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) に似た仕様とします．\n",
        "\n",
        "訓練データ `X_train` とクラスラベル `y_train` からモデルを作成し，`X_val` のクラスラベルの予測 `y_pred` を得たり，正答率を表示するには，\n",
        "```\n",
        "    model = myMultinomialNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    print(model.score(X_val, y_val))\n",
        "```\n",
        "のように使うことを想定します．\n",
        "- 訓練データの次元数`d`，個数`n`の場合，`X_train` と `y_train` はそれぞれ shape が `(n,d)` と `(n,)` のNumPy配列です．文書分類のBoW表現では，`d` は単語数，`n` は文書数，`X_train` の要素は単語の出現回数です．\n",
        "- `myMultinomialNB` の引数 `alpha` はスムージングのパラメタです．デフォルトは1.0で，いかなる単語も少なくとも `alpha` 回出現したものとして生成確率を計算します．"
      ],
      "metadata": {
        "id": "UBYXMS7ICxHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class myMultinomialNB:\n",
        "\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.class_count_ = None        # 各クラスの出現回数                    # (n_classes,)\n",
        "        self.class_log_prior_ = None    # 各クラスの出現頻度（事前確率）の対数  # (n_classes,)\n",
        "        self.feature_count_ = None      # 各クラスにおける単語の出現回数        # (n_classes,n_features)  \n",
        "        self.feature_log_prob_ = None   # 各クラスにおける単語の生成確率の対数  # (n_classes,n_features)  \n",
        "\n",
        "\n",
        "    # 学習：上記の None に該当する値をすべて求めます．\n",
        "    # 確率はすべて対数で記録します．\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_classes = len(self.classes_)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self.class_count_ =  np.zeros(n_classes, dtype=np.int64)\n",
        "        self.feature_count_ = np.zeros((n_classes, n_features), dtype=np.int64)\n",
        "\n",
        "        for cid, c in enumerate(self.classes_):\n",
        "            X_c = X[y==c]\n",
        "            self.class_count_[cid] = X_c.shape[0]\n",
        "            self.feature_count_[cid, :] = X_c.sum(axis=0)\n",
        "\n",
        "        #### prob_c = ''' 各クラスの出現頻度（事前確率）を計算しましょう．全文書数は n_samples です． '''\n",
        "\n",
        "        # 確率を対数にして記録します．\n",
        "        self.class_log_prior_ = np.log(prob_c)\n",
        "\n",
        "        #### smoothed_feature_count = ''' 各クラスにおける単語の出現回数に self.alpha を加えたものを計算します． '''\n",
        "        #### n_words_c = ''' 各クラスにおける単語の出現回数の総和を計算します．'''\n",
        "\n",
        "        # 確率を対数にして記録します．\n",
        "        self.feature_log_prob_ = np.log(smoothed_feature_count)\n",
        "        for cid in range(len(self.classes_)):\n",
        "            self.feature_log_prob_[cid,:] -= np.log(n_words_c[cid])\n",
        "\n",
        "\n",
        "    # 推論：最大の事後確率のクラスを答えます．\n",
        "    # x の要素は，ある文書の各単語の出現回数です．\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        # 各クラスの事後確率を計算して posteriors に追加します．\n",
        "        # for cid, c in enumerate(self.classes_):\n",
        "        for cid in range(len(self.classes_)):\n",
        "            prior_c = self.class_log_prior_[cid]\n",
        "            posterior_c = np.sum(x * self.feature_log_prob_[cid,:])     # sum_j log P(word_j | cid)\n",
        "            posteriors.append(self.class_log_prior_[cid] + posterior_c) # log P(words, cid)\n",
        "\n",
        "        # 最大事後確率のクラスを返します．\n",
        "        return self.classes_[np.argmax(posteriors)]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return (y == y_pred).sum() / len(y)"
      ],
      "metadata": {
        "id": "0wgc_5GpGjyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実装した`myMultinomialNB`で識別します．"
      ],
      "metadata": {
        "id": "5Bk1HovpADsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = myMultinomialNB()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Accuracy on training data: \", model.score(X, y))\n",
        "print(\"Accuracy on validation data: \", model.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "C95O9uF8MmE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "## 発展課題（任意）\n",
        "\n",
        "    1. BoW表現を用いた決定木ではどの単語が識別的な特徴になるか，Exampleの各例で観察してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "    2. Stemmingとは，どのような処理のことでしょうか．この処理ができる自然言語処理（natural language processing; NLP）のツールを調べ，\n",
        "       試用した結果を報告してください．\n",
        "\n",
        "（ここに回答を書いてください）"
      ],
      "metadata": {
        "id": "jVhtoi5sM0Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 決定木\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='entropy', splitter='best', random_state=0, max_depth=8)\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Accuracy on training data: \", model.score(X, y))\n",
        "print(\"Accuracy on validation data: \", model.score(X_val, y_val))\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,12))\n",
        "arts = plot_tree(model, filled=True, feature_names=vectorizer.get_feature_names_out(), #class_names=model.classes_,\n",
        "          impurity=False, rounded=True, fontsize=8, class_names=model.classes_);"
      ],
      "metadata": {
        "id": "10WIBl6UPvpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRUeztQpn2op",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title 混同行列（行：正解，列：予測）\n",
        "from sklearn import metrics\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "cm = metrics.confusion_matrix(y_val, y_pred)\n",
        "print(cm)\n",
        "\n",
        "#print(cm.sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "お疲れさまでした．"
      ],
      "metadata": {
        "id": "SiU6AigwuemU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "h4Xq7nKCMeqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix: Word-splitting together with [stemming](https://en.wikipedia.org/wiki/Stemming) in alphabetical languages\n",
        "\n",
        "[nltk.stem.snowball module](https://www.nltk.org/api/nltk.stem.snowball.html)\n",
        ", [nltk.stem.SnowballStemmer](https://tedboy.github.io/nlps/generated/generated/nltk.stem.SnowballStemmer.html)"
      ],
      "metadata": {
        "id": "mykO5spcw9FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk.stem\n",
        "\n",
        "language = 'english'    # 'arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'\n",
        "stemmer = nltk.stem.SnowballStemmer(language)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
        "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"The elephant sneezed at the sight of potatoes.\",\n",
        "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "    \"Wondering, she opened the door to the studio.\",\n",
        "]\n",
        "\n",
        "vectorizer = StemmedCountVectorizer(analyzer=\"word\", stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "uyrCrcuWwbNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix: Word-splitting and text segmentation in East Asian languages\n",
        "\n",
        "https://investigate.ai/text-analysis/splitting-words-in-east-asian-languages/#Chinese:-jieba"
      ],
      "metadata": {
        "id": "JpdykOAjq0M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_jp = [\n",
        "    \"ペニーは鮮やかな青い魚を買った。\",\n",
        "    \"ペニーは明るい青とオレンジの魚を買った。\",\n",
        "    \"猫は店で魚を食べました。\",\n",
        "    \"ペニーは店に行きました。ペニーは虫を食べました。ペニーは魚を見ました。\",\n",
        "    \"ペニーは魚です\"\n",
        "]\n",
        "\n",
        "!pip install -q nagisa\n",
        "import nagisa\n",
        "\n",
        "print(nagisa.tagging(corpus_jp[0]).words)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tokenizer_jp = lambda text: nagisa.tagging(text).words\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer_jp)\n",
        "X = vectorizer.fit_transform(corpus_jp)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "-RzYMw9BooZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_zh = [\n",
        "  '翠花买了浅蓝色的鱼',\n",
        "  '翠花买了浅蓝橙色的鱼',\n",
        "  '猫在商店吃了一条鱼',\n",
        "  '翠花去了商店。翠花买了一只虫子。翠花看到一条鱼',\n",
        "  '翠花是鱼'  \n",
        "]\n",
        "\n",
        "import jieba\n",
        "\n",
        "print(jieba.lcut('翠花买了浅蓝色的鱼'))\n",
        "\n",
        "tokenizer_zh = lambda text: jieba.lcut(text)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer_zh)\n",
        "X = vectorizer.fit_transform(corpus_zh)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "Uixl33fioytW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thai\n",
        "!pip install -q tltk\n",
        "\n",
        "import tltk\n",
        "corpus_th = [\n",
        "    \"\"\"สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ \n",
        "ในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์\"\"\"\n",
        "]\n",
        "\n",
        "#print(tltk.nlp.pos_tag(phrase))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tokenizer_th = lambda text: [(yield word[0]) for piece in pieces for word in piece]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer_th)\n",
        "X = vectorizer.fit_transform(corpus_th)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "qy23M749ozk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vietnamese\n",
        "#!pip install -q pyvi\n",
        "#from pyvi import ViTokenizer\n",
        "\n",
        "corpus_vi = [\n",
        "    \"Trường đại học bách khoa hà nội\"\n",
        "]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus_vi)    # (number of docs, number of words)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "Zmo0k5W7srj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}